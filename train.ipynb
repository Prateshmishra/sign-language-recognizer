{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prateshmishra/sign-language-recognizer/blob/main/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6ek-VFONkHV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc15b7f9-153f-41b9-e2dd-80759866d342"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Loading images from folder  A  has started.\n",
            "Loading images from folder  B  has started.\n",
            "Loading images from folder  C  has started.\n",
            "Loading images from folder  D  has started.\n",
            "Loading images from folder  E  has started.\n",
            "Loading images from folder  F  has started.\n",
            "Loading images from folder  G  has started.\n",
            "Loading images from folder  H  has started.\n",
            "Loading images from folder  I  has started.\n",
            "Loading images from folder  J  has started.\n",
            "Loading images from folder  K  has started.\n",
            "Loading images from folder  L  has started.\n",
            "Loading images from folder  M  has started.\n",
            "Loading images from folder  N  has started.\n",
            "Loading images from folder  O  has started.\n",
            "Loading images from folder  P  has started.\n",
            "Loading images from folder  Q  has started.\n",
            "Loading images from folder  R  has started.\n",
            "Loading images from folder  S  has started.\n",
            "Loading images from folder  T  has started.\n",
            "Loading images from folder  U  has started.\n",
            "Loading images from folder  V  has started.\n",
            "Loading images from folder  W  has started.\n",
            "Loading images from folder  X  has started.\n",
            "Loading images from folder  Y  has started.\n",
            "Loading images from folder  Z  has started.\n",
            "Loading images from folder  space  has started.\n",
            "Loading images from folder  del  has started.\n",
            "Loading images from folder  nothing  has started.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=\"true\")\n",
        "\n",
        "# import os\n",
        "# os.chdir('/content/drive/My Drive/new')\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  import zipfile\n",
        "\n",
        "  # Open the ZIP file\n",
        "  # with zipfile.ZipFile('/content/drive/My Drive/new/archive.zip', 'r') as zip_ref:\n",
        "  with zipfile.ZipFile('/content/drive/My Drive/niu/ARCHIVE.zip', 'r') as zip_ref:\n",
        "      # Extract a file named 'example.txt' from the archive to the current directory\n",
        "      folders = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H',\n",
        "                 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P',\n",
        "                 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X',\n",
        "                 'Y', 'Z', 'space', 'del', 'nothing']\n",
        "        \n",
        "      for folder in folders:\n",
        "        # index += 1\n",
        "        # xtract_path='/content/drive/My Drive/new/hello/asl_alphabet_train/asl_alphabet_train/' + folder + '/'\n",
        "        xtract_path='/content/drive/My Drive/niu/asl_alphabet_train/asl_alphabet_train/' + folder + '/'\n",
        "\n",
        "        import os\n",
        "        if not os.path.exists(xtract_path):\n",
        "          os.makedirs(xtract_path)\n",
        "        \n",
        "        print(\"Loading images from folder \", folder ,\" has started.\")\n",
        "        # for image in os.listdir(self.dir + '/' + folder):\n",
        "\n",
        "          # img = cv2.imread(self.dir + '/' + folder + '/' + image, 0)\n",
        "        for i in range(1,3001):\n",
        "          zip_ref.extract('asl_alphabet_train/asl_alphabet_train/' + \n",
        "                          folder + '/' + folder + str(i) + '.jpg', xtract_path)\n",
        "#   import zipfile\n",
        "#   with zipfile.ZipFile('/content/drive/My Drive/archive.zip', 'r') as zip_ref:\n",
        "#       zip_ref.extractall('.')\n",
        "  # !unzip \"/content/drive/My Drive/new/archive.zip\" -d \"/content/drive/My Drive/new/ds/\"\n",
        "  # !ls\n",
        "  # import zipfile\n",
        "\n",
        "  # Open the ZIP file\n",
        "  # with zipfile.ZipFile('/content/drive/My Drive/new/archive.zip', 'r') as zip_ref:\n",
        "      # Extract a file named 'example.txt' from the archive to the current directory\n",
        "      # zip_ref.extract('example.txt', '.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5tv4QFpJWdx"
      },
      "outputs": [],
      "source": [
        "# # puneet id\n",
        "\n",
        "# from tensorflow.keras.models import Sequential, load_model\n",
        "# from tensorflow.keras.layers import Convolution2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# from tensorflow.keras.preprocessing.image import load_img, img_to_array, array_to_img\n",
        "# from tensorflow.keras.utils import to_categorical\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# import numpy as np\n",
        "# import os\n",
        "# import cv2\n",
        "\n",
        "\n",
        "# class Model:\n",
        "\n",
        "#   classifier = None\n",
        "#   def __init__(self, Type):\n",
        "#     self.classifier = Type\n",
        "    \n",
        "  \n",
        "#   def build_model(classifier):\n",
        "#     print(\"inside build_model\")\n",
        "    \n",
        "\n",
        "#     classifier.add(Convolution2D(128, (3, 3), input_shape=(64, 64, 1), activation='relu'))\n",
        "\n",
        "#     classifier.add(Convolution2D(256, (3, 3), activation='relu'))\n",
        "#     classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "#     classifier.add(Convolution2D(256, (3, 3), activation='relu'))\n",
        "#     classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    \n",
        "#     classifier.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "#     classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "#     classifier.add(Dropout(0.5))\n",
        "\n",
        "#     classifier.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "#     classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "#     classifier.add(Dropout(0.5))\n",
        "\n",
        "#     classifier.add(Flatten())\n",
        "\n",
        "#     classifier.add(Dropout(0.5))\n",
        "    \n",
        "#     classifier.add(Dense(1024, activation='relu'))\n",
        "    \n",
        "\n",
        "#     classifier.add(Dense(29, activation='softmax'))\n",
        "#     print(\"outside build_model\")\n",
        "\n",
        "#     return classifier\n",
        "\n",
        "#   def save_classifier(path, classifier):\n",
        "#     classifier.save('/content/drive/MyDrive/niu/13-epoch.h5')\n",
        "\n",
        "#   def load_classifier(path):\n",
        "#     classifier = load_model(path)\n",
        "#     return classifier\n",
        "\n",
        "#   def predict(classes, classifier, img):\n",
        "#     img = cv2.resize(img, (64, 64))\n",
        "#     img = img_to_array(img)\n",
        "#     img = np.expand_dims(img, axis=0)\n",
        "#     img = img/255.0\n",
        "\n",
        "#     pred = classifier.predict(img)\n",
        "#     return classes[np.argmax(pred)], pred\n",
        "    \n",
        "\n",
        "# class DataGatherer:\n",
        "\n",
        "#   def __init__(self, *args):\n",
        "#     if len(args) > 0:\n",
        "#       self.dir = args[0]\n",
        "#     elif len(args) == 0:\n",
        "#       self.dir = \"\"\n",
        "\n",
        "\n",
        "#   #this function loads the images along with their labels and apply\n",
        "#   #pre-processing function on the images and finally split them into train and\n",
        "#   #test dataset\n",
        "#   def load_images(self):\n",
        "#     images = []\n",
        "#     labels = []\n",
        "#     index = -1\n",
        "#     folders = sorted(os.listdir(self.dir))\n",
        "    \n",
        "#     for folder in folders:\n",
        "#       index += 1\n",
        "      \n",
        "#       print(\"Loading images from folder \", folder ,\" has started.\")\n",
        "#       for image in os.listdir(self.dir + '/' + folder + '/asl_alphabet_train/asl_alphabet_train/' + folder + '/'):\n",
        "\n",
        "#         img = cv2.imread(self.dir + '/' + folder +\n",
        "#                          '/asl_alphabet_train/asl_alphabet_train/' + folder + '/' + image, 0)\n",
        "#         # print(type(img))\n",
        "        \n",
        "#         img = self.edge_detection(img)\n",
        "#         img = cv2.resize(img, (64, 64))\n",
        "#         img = img_to_array(img)\n",
        "\n",
        "#         images.append(img)\n",
        "#         labels.append(index)\n",
        "\n",
        "#     images = np.array(images)\n",
        "#     images = images.astype('float32')/255.0\n",
        "#     labels = to_categorical(labels)\n",
        "\n",
        "\n",
        "#     x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.1)\n",
        "#     # print(\"outside load_images\")\n",
        "\n",
        "#     return x_train, x_test, y_train, y_test\n",
        "\n",
        "#   def edge_detection(self, image):\n",
        "#     # print(image)\n",
        "#     # print(\"inside edge_detection\")\n",
        "#     minValue = 70\n",
        "#     blur = cv2.GaussianBlur(image,(5,5),2)\n",
        "#     th3 = cv2.adaptiveThreshold(blur,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY_INV,11,2)\n",
        "#     ret, res = cv2.threshold(th3, minValue, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
        "#     return res\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pratesh id\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array, array_to_img\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "\n",
        "class Model:\n",
        "\n",
        "  classifier = None\n",
        "  def __init__(self, Type):\n",
        "    self.classifier = Type\n",
        "    \n",
        "  \n",
        "  def build_model(classifier):\n",
        "    print(\"inside build_model\")\n",
        "    \n",
        "\n",
        "    classifier.add(Convolution2D(128, (3, 3), input_shape=(64, 64, 1), activation='relu'))\n",
        "\n",
        "    classifier.add(Convolution2D(256, (3, 3), activation='relu'))\n",
        "    classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    classifier.add(Convolution2D(256, (3, 3), activation='relu'))\n",
        "    classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    \n",
        "    classifier.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "    classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    classifier.add(Dropout(0.5))\n",
        "\n",
        "    classifier.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "    classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    classifier.add(Dropout(0.5))\n",
        "\n",
        "    classifier.add(Flatten())\n",
        "\n",
        "    classifier.add(Dropout(0.5))\n",
        "    \n",
        "    classifier.add(Dense(1024, activation='relu'))\n",
        "    \n",
        "\n",
        "    classifier.add(Dense(29, activation='softmax'))\n",
        "    print(\"outside build_model\")\n",
        "\n",
        "    return classifier\n",
        "\n",
        "  def save_classifier(path, classifier):\n",
        "    classifier.save('/content/drive/MyDrive/new/7-epoch.h5')\n",
        "\n",
        "  def load_classifier(path):\n",
        "    classifier = load_model(path)\n",
        "    return classifier\n",
        "\n",
        "  def predict(classes, classifier, img):\n",
        "    img = cv2.resize(img, (64, 64))\n",
        "    img = img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img = img/255.0\n",
        "\n",
        "    pred = classifier.predict(img)\n",
        "    return classes[np.argmax(pred)], pred\n",
        "    \n",
        "\n",
        "class DataGatherer:\n",
        "\n",
        "  def __init__(self, *args):\n",
        "    if len(args) > 0:\n",
        "      self.dir = args[0]\n",
        "    elif len(args) == 0:\n",
        "      self.dir = \"\"\n",
        "\n",
        "\n",
        "  #this function loads the images along with their labels and apply\n",
        "  #pre-processing function on the images and finally split them into train and\n",
        "  #test dataset\n",
        "  def load_images(self):\n",
        "    images = []\n",
        "    labels = []\n",
        "    index = -1\n",
        "    folders = sorted(os.listdir(self.dir))\n",
        "    \n",
        "    for folder in folders:\n",
        "      index += 1\n",
        "      \n",
        "      print(\"Loading images from folder \", folder ,\" has started.\")\n",
        "      for image in os.listdir(self.dir + '/' + folder + '/asl_alphabet_train/asl_alphabet_train/' + folder + '/'):\n",
        "\n",
        "        img = cv2.imread(self.dir + '/' + folder +\n",
        "                         '/asl_alphabet_train/asl_alphabet_train/' + folder + '/' + image, 0)\n",
        "        # print(type(img))\n",
        "        \n",
        "        img = self.edge_detection(img)\n",
        "        img = cv2.resize(img, (64, 64))\n",
        "        img = img_to_array(img)\n",
        "\n",
        "        images.append(img)\n",
        "        labels.append(index)\n",
        "\n",
        "    images = np.array(images)\n",
        "    images = images.astype('float32')/255.0\n",
        "    labels = to_categorical(labels)\n",
        "\n",
        "\n",
        "    x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.1)\n",
        "    # print(\"outside load_images\")\n",
        "\n",
        "    return x_train, x_test, y_train, y_test\n",
        "\n",
        "  def edge_detection(self, image):\n",
        "    # print(image)\n",
        "    # print(\"inside edge_detection\")\n",
        "    minValue = 70\n",
        "    blur = cv2.GaussianBlur(image,(5,5),2)\n",
        "    th3 = cv2.adaptiveThreshold(blur,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY_INV,11,2)\n",
        "    ret, res = cv2.threshold(th3, minValue, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
        "    return res\n",
        "\n"
      ],
      "metadata": {
        "id": "3SvhhDUHNf5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YyRQhxvmSMc-",
        "outputId": "0fe569f1-97f1-4e96-ba9d-b7a05027614c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbbqUImIJbx6",
        "outputId": "88c60716-51fc-4ece-85be-83528e94bef2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e963e6cd0f15>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0;31m# data_gatherer = DataGatherer('/content/drive/My Drive/new/ds/asl_alphabet_train/asl_alphabet_train')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0mdata_gatherer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataGatherer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/new/asl_alphabet_train/asl_alphabet_train/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_gatherer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'DataGatherer' is not defined"
          ]
        }
      ],
      "source": [
        "# pratesh id\n",
        "# from cnn import Model, DataGatherer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from math import ceil\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=\"true\")\n",
        "\n",
        "# import os\n",
        "# os.chdir('/content/drive/My Drive/new')\n",
        "\n",
        "# import zipfile\n",
        "# with zipfile.ZipFile('/content/drive/My Drive/archive.zip', 'r') as zip_ref:\n",
        "#     zip_ref.extractall('.')\n",
        "\n",
        "# training_dir = 'D:\\\\archive\\\\asl_alphabet_train\\\\asl_alphabet_train'\n",
        "\n",
        "#loading the images from training directory\n",
        "with tf.device('/device:GPU:0'):  \n",
        "\n",
        "  # data_gatherer = DataGatherer('/content/drive/My Drive/new/ds/asl_alphabet_train/asl_alphabet_train')\n",
        "  data_gatherer = DataGatherer('/content/drive/My Drive/new/asl_alphabet_train/asl_alphabet_train/')\n",
        "\n",
        "  x_train, x_test, y_train, y_test = data_gatherer.load_images()\n",
        "\n",
        "\n",
        "  batch_size = 64\n",
        "  training_size = x_train.shape[0]\n",
        "  test_size = x_test.shape[0]\n",
        "\n",
        "\n",
        "  #computing steps and validation steps per epoch according to training\n",
        "  #and testing size\n",
        "  compute_steps_per_epoch = lambda x: int(ceil(1. * x/batch_size))\n",
        "  steps_per_epoch = compute_steps_per_epoch(training_size)\n",
        "  val_steps = compute_steps_per_epoch(test_size)\n",
        "\n",
        "\n",
        "  #build the model\n",
        "  classifier = Model(Sequential()).classifier\n",
        "  classifier = Model.build_model(classifier)\n",
        "\n",
        "  classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  #train the model\n",
        "  history = classifier.fit(\n",
        "    x_train, y_train,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=14,\n",
        "    validation_data=(x_test, y_test),\n",
        "    validation_steps=val_steps)\n",
        "\n",
        "  #plot accuracy graph\n",
        "  plt.figure(figsize=(8,5))\n",
        "\n",
        "  plt.plot(history.history['accuracy'], label='train_accuracy',)\n",
        "  plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.legend()\n",
        "  plt.title(\"classifier\")\n",
        "\n",
        "  plt.show();\n",
        "\n",
        "  #run the below line to save the classifier\n",
        "  # Model.save('/content/drive/My Drive/new', classifier)\n",
        "  Model.save_classifier('/content/drive/My Drive/niu', classifier)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uaTw3sfgD8pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# puneet id\n",
        "\n",
        "# from cnn import Model, DataGatherer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from math import ceil\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=\"true\")\n",
        "\n",
        "# import os\n",
        "# os.chdir('/content/drive/My Drive/new')\n",
        "\n",
        "# import zipfile\n",
        "# with zipfile.ZipFile('/content/drive/My Drive/archive.zip', 'r') as zip_ref:\n",
        "#     zip_ref.extractall('.')\n",
        "\n",
        "# training_dir = 'D:\\\\archive\\\\asl_alphabet_train\\\\asl_alphabet_train'\n",
        "\n",
        "#loading the images from training directory\n",
        "with tf.device('/device:GPU:0'):  \n",
        "\n",
        "  # data_gatherer = DataGatherer('/content/drive/My Drive/new/ds/asl_alphabet_train/asl_alphabet_train')\n",
        "  data_gatherer = DataGatherer('/content/drive/My Drive/niu/asl_alphabet_train/asl_alphabet_train')\n",
        "\n",
        "  x_train, x_test, y_train, y_test = data_gatherer.load_images()\n",
        "\n",
        "\n",
        "  batch_size = 64\n",
        "  training_size = x_train.shape[0]\n",
        "  test_size = x_test.shape[0]\n",
        "\n",
        "\n",
        "  #computing steps and validation steps per epoch according to training\n",
        "  #and testing size\n",
        "  compute_steps_per_epoch = lambda x: int(ceil(1. * x/batch_size))\n",
        "  steps_per_epoch = compute_steps_per_epoch(training_size)\n",
        "  val_steps = compute_steps_per_epoch(test_size)\n",
        "\n",
        "\n",
        "  #build the model\n",
        "  classifier = Model(Sequential()).classifier\n",
        "  classifier = Model.build_model(classifier)\n",
        "\n",
        "  classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  #train the model\n",
        "  history = classifier.fit(\n",
        "    x_train, y_train,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=9,\n",
        "    validation_data=(x_test, y_test),\n",
        "    validation_steps=val_steps)\n",
        "\n",
        "  #plot accuracy graph\n",
        "  plt.figure(figsize=(8,5))\n",
        "\n",
        "  plt.plot(history.history['accuracy'], label='train_accuracy',)\n",
        "  plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.legend()\n",
        "  plt.title(\"classifier\")\n",
        "\n",
        "  plt.show();\n",
        "\n",
        "  #run the below line to save the classifier\n",
        "  # Model.save('/content/drive/My Drive/new', classifier)\n",
        "  Model.save_classifier('/content/drive/My Drive/niu', classifier)"
      ],
      "metadata": {
        "id": "DKAHKaZEM9gh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cab83ad7-66fb-448b-f032-e12402d70f77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Loading images from folder  A  has started.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}